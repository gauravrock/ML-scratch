{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates:\n",
    "1. sigmoid function mistake\n",
    "2. Bias not added\n",
    "3. backward derivative\n",
    "4. forward derivative\n",
    "5. dont need NN until we have lots of data and features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.layer_count = 1\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def W_initializer(self, shape):\n",
    "        \n",
    "        '''\n",
    "        This function will return a matrix of normally distributed random values of 0 mean and 1 std. \n",
    "        Parameters:\n",
    "        shape: List, Tuple\n",
    "            Shape will be a tuple of two values input shape and output shape values. \n",
    "        '''\n",
    "        return np.random.randn(shape[0], shape[1])\n",
    "    \n",
    "    def Bias_initializer(self, shape):\n",
    "        '''\n",
    "        This function will return a vector of normally distributed random values of 0 mean and 1 std. \n",
    "        parameters\n",
    "        shape: int\n",
    "            Output Vector size\n",
    "        '''\n",
    "        return np.random.normal(size = shape)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "        This function will return sigmoid activated value. \n",
    "        parameters\n",
    "        x:\n",
    "            x will be the power value, a matrix multiplication of theta vector and feature vector. \n",
    "        '''\n",
    "        #print(x)\n",
    "        return 1 / (1 + np.exp(-x))   # 1 / (1 * np.exp(-x)) this is mot multiply\n",
    "    \n",
    "    def relu(self, x):\n",
    "        '''\n",
    "        ReLU is an activation function\n",
    "        '''\n",
    "        return np.maximum(x, np.zeros(x.shape))\n",
    "    \n",
    "    def soft_max(self, x):\n",
    "        \n",
    "        '''\n",
    "        Soft max is an activation function vector. \n",
    "        parameters\n",
    "        x: np.array\n",
    "            x will be the power values\n",
    "        '''\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis = 1)[:, None]\n",
    "    \n",
    "    def add_layer(self, in_shape, out_shape, activation = 'relu'):\n",
    "        \n",
    "        '''\n",
    "        This funtion will add a layer into the neural network. Assigning all the parameters to the neural network. \n",
    "        Also assigning the activation function into the paramters. \n",
    "        '''\n",
    "        \n",
    "        #self.parameters['layer: ' + str(self.layer_count)] = {'W': self.W_initializer((in_shape, out_shape)), 'b': np.ones((out_shape))}\n",
    "        \n",
    "        self.parameters['layer: ' + str(self.layer_count)] = {'W': self.W_initializer((in_shape, out_shape)), 'b': self.Bias_initializer((out_shape))}\n",
    "        \n",
    "        \n",
    "        if (activation == 'relu'):\n",
    "            self.parameters['layer: ' + str(self.layer_count)]['Activation'] = self.relu\n",
    "        elif (activation == 'sigmoid'):\n",
    "            self.parameters['layer: ' + str(self.layer_count)]['Activation'] = self.sigmoid\n",
    "        elif (activation == 'soft_max'):\n",
    "            self.parameters['layer: ' + str(self.layer_count)]['Activation'] = self.soft_max\n",
    "            \n",
    "        self.layer_count += 1\n",
    "    \n",
    "    def dense_layer(self, x, parameters):\n",
    "        \n",
    "        '''\n",
    "        Dense Layer is the matrix multiplication of Weight matrix and feature vector then adding the bias in it. \n",
    "        Activation function will call after operation. \n",
    "        '''\n",
    "        W = parameters['W']\n",
    "        bias = parameters['b']\n",
    "        activation = parameters['Activation']\n",
    "        Z = np.matmul(x, W) + bias\n",
    "        return activation(Z)\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        '''\n",
    "        Forward Pass\n",
    "        '''\n",
    "        result = x\n",
    "        self.results['A0'] = result\n",
    "        for i in range(1, self.layer_count):\n",
    "            result = self.dense_layer(result, self.parameters['layer: ' + str(i)])\n",
    "            self.results['A' + str(i)] = result\n",
    "        return result\n",
    "          \n",
    "    \n",
    "    \n",
    "    \n",
    "    def del_forward_pass(self):\n",
    "        '''\n",
    "        This function will calculate the derivatives for the forward pass. \n",
    "        '''\n",
    "        \n",
    "        dev_forpass = {}\n",
    "        \n",
    "        # adding \n",
    "        bias = np.ones([1 , self.results['A' + str(0)].shape[0]])\n",
    "        \n",
    "        for i in range(1, self.layer_count):\n",
    "            act = self.parameters['layer: ' + str(i)]['Activation'] ## Taking activation for calculating different derivatives\n",
    "            result = self.results['A' + str(i - 1)]\n",
    "            if (act == self.relu):\n",
    "                dev_forpass['layer: ' + str(i)] = {'dW': result.T, 'db': result.T}\n",
    "            \n",
    "            elif (act == self.sigmoid): \n",
    "                ######### YOUR CODE ###########\n",
    "                #sig = self.sigmoid(result.T)\n",
    "                #dev_forpass['layer: ' + str(i)] = {'dW': ((sig * (1 - sig)) * result.T), 'db': (1 - sig)}\n",
    "                \n",
    "                ########### CHANGE CODE #############\n",
    "                \n",
    "                # results already contains output of sigmoid so you dont need to pass your data through \n",
    "                # sigmoid function again.  results = {A0 , A1=sig(A0) , A2=sig(A1) }\n",
    "                \n",
    "                # derivative is also wrong\n",
    "                sig = self.results['A' + str(i)]\n",
    "                dev_forpass['layer: ' + str(i)] = {'dW': np.matmul(result.T , (sig * (1 - sig))), 'db': np.matmul( bias , (sig * (1 - sig)))}\n",
    "                 \n",
    "        return dev_forpass\n",
    "    \n",
    "    def del_backward_pass(self ,  error):\n",
    "        \n",
    "        '''\n",
    "        In backward propogation we have to calculate all the values \n",
    "        \n",
    "        '''\n",
    "        #dev_backpass = {}\n",
    "        #theta_mul = np.ones((in_shape, 1)) * error ## Last Layer dL / dA\n",
    "        \n",
    "        #for i in range(self.layer_count - 1, 1, -1):\n",
    "         #   dev_backpass['layer: '+ str(i)] = {'dW': theta_mul, 'db': 1} ## Storing del Activations \n",
    "          #  theta_val = self.parameters['layer: ' + str(i)] ## Taking thetas of l + 1 layer\n",
    "          #  theta_mul =  theta_mul @ theta_val['W'].T ## Calculating del Activation \n",
    "\n",
    "        #dev_backpass['layer: '+ str(i - 1)] = {'dW': theta_mul, 'db': 1}\n",
    "        #return dev_backpass\n",
    "    \n",
    "\n",
    "        dev_backpass = {}\n",
    "        dev_backpass['layer: ' + str(self.layer_count-1)] = {'dW': error , 'db': error }  \n",
    "                \n",
    "        for i in range(self.layer_count - 3, -1, -1):   # 2 = 3 , 1 = 2 , 0 = 1\n",
    "            theta_val = self.parameters['layer: ' + str(i+1)]                   ## Taking thetas of l + 1 layer\n",
    "            theta_mul =  error * theta_val['W']                                 ## Calculating del Activation \n",
    "            bias_mul = error * theta_val['b']\n",
    "            dev_backpass['layer: '+ str(i+1)] = {'dW': theta_mul, 'db': bias_mul} ## Storing del Activations \n",
    "        \n",
    "        return dev_backpass\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        return self.forward_pass(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.forward_pass(x))\n",
    "    \n",
    "    def GradientDescentOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        loss_dir = []\n",
    "        for _ in range(epoch):\n",
    "            result = self.forward_pass(x)\n",
    "            del_loss  = self.biclass_cross_entropy( result , y)\n",
    "            del_forward = self.del_forward_pass()\n",
    "            \n",
    "            #del_backward = self.del_backward_pass(x.shape[0], del_loss)\n",
    "            del_backward = self.del_backward_pass( del_loss)\n",
    "            \n",
    "            for i in range(1, self.layer_count):\n",
    "                self.parameters['layer: ' + str(i)]['W'] -= (1/x.shape[0])*(alpha * (del_forward['layer: ' + str(i)]['dW'] * del_backward['layer: ' + str(i)]['dW']))\n",
    "                #self.parameters['layer: ' + str(i)]['b'] -= alpha * (del_forward['layer: ' + str(i)]['db'] * del_backward['layer: ' + str(i)]['db'])\n",
    "\n",
    "            loss = self.Biclass_Loss(result, y)\n",
    "            print ('Loss: ', loss)\n",
    "            loss_dir.append(loss)\n",
    "        return loss_dir\n",
    "    \n",
    "    def AdamOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        pass\n",
    "    \n",
    "    def AdagradeOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        pass\n",
    "        \n",
    "    def RMSpropOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        pass\n",
    "    \n",
    "    def MomentumUpdateOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        pass\n",
    "    \n",
    "    def NesterovMomentumOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        pass\n",
    "    \n",
    "    def softmax_cross_entropy(self, logits, labels):\n",
    "        return (- np.mean( labels - np.log(self.soft_max(logits))))\n",
    "    \n",
    "    def Biclass_Loss(self, x, y):\n",
    "        sig = self.sigmoid(x)\n",
    "        return -np.mean( (y * np.log(sig)) + ((1 - y) * np.log(1 - sig)))\n",
    "    \n",
    "    def multiclass_Loss(self, x, y):\n",
    "        return -np.mean(y * np.log(self.soft_max(x)))\n",
    "    \n",
    "    def biclass_cross_entropy(self, x, y):\n",
    "        return -np.mean((np.log(self.sigmoid(x)) - y) * x)\n",
    "        \n",
    "    def fit(self, x, y, learning_rate, optimizer = 'grad_dst', epoch = 100):\n",
    "        \n",
    "        if (optimizer == 'grad_dst'):\n",
    "            self.GradientDescentOptimizer(x, y, learning_rate, epoch)\n",
    "        elif (optimizer == 'adam'):\n",
    "            pass\n",
    "        elif (optimizer == 'rmsprop'):\n",
    "            pass\n",
    "        elif(optimizer == 'adagrade'):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.add_layer(3, 2, 'sigmoid')\n",
    "nn.add_layer(2, 2, 'sigmoid')\n",
    "nn.add_layer(2, 2, 'sigmoid')\n",
    "nn.add_layer(2, 2, 'sigmoid')\n",
    "nn.add_layer(2, 2, 'sigmoid')\n",
    "nn.add_layer(2, 2, 'sigmoid')\n",
    "\n",
    "nn.add_layer(2, 1, 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.normal(5, 2, (10000, 3))\n",
    "y = np.random.randint(0, 2, (10000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7623442134009567\n",
      "Loss:  0.7619306118893472\n",
      "Loss:  0.7615140288960971\n",
      "Loss:  0.7610945010445517\n",
      "Loss:  0.7606720674019561\n",
      "Loss:  0.760246769513479\n",
      "Loss:  0.7598186514332927\n",
      "Loss:  0.7593877597524882\n",
      "Loss:  0.7589541436236046\n",
      "Loss:  0.7585178547815596\n",
      "Loss:  0.7580789475607626\n",
      "Loss:  0.7576374789082013\n",
      "Loss:  0.7571935083922983\n",
      "Loss:  0.7567470982073359\n",
      "Loss:  0.7562983131732616\n",
      "Loss:  0.755847220730693\n",
      "Loss:  0.7553938909309506\n",
      "Loss:  0.754938396420964\n",
      "Loss:  0.7544808124229055\n",
      "Loss:  0.754021216708418\n",
      "Loss:  0.7535596895673236\n",
      "Loss:  0.7530963137707016\n",
      "Loss:  0.7526311745282461\n",
      "Loss:  0.7521643594398071\n",
      "Loss:  0.7516959584410284\n",
      "Loss:  0.7512260637429832\n",
      "Loss:  0.7507547697656975\n",
      "Loss:  0.7502821730654232\n",
      "Loss:  0.7498083722555029\n",
      "Loss:  0.749333467920646\n",
      "Loss:  0.7488575625244318\n",
      "Loss:  0.7483807603098973\n",
      "Loss:  0.7479031671931718\n",
      "Loss:  0.7474248906502886\n",
      "Loss:  0.7469460395975834\n",
      "Loss:  0.7464667242663862\n",
      "Loss:  0.7459870560730367\n",
      "Loss:  0.7455071474854646\n",
      "Loss:  0.7450271118876175\n",
      "Loss:  0.7445470634428504\n",
      "Loss:  0.7440671169570139\n",
      "Loss:  0.7435873877415088\n",
      "Loss:  0.7431079914761364\n",
      "Loss:  0.742629044071275\n",
      "Loss:  0.7421506615288294\n",
      "Loss:  0.7416729598015004\n",
      "Loss:  0.7411960546501604\n",
      "Loss:  0.740720061499401\n",
      "Loss:  0.7402450952915767\n",
      "Loss:  0.7397712703398727\n",
      "Loss:  0.7392987001810372\n",
      "Loss:  0.7388274974284827\n",
      "Loss:  0.7383577736264596\n",
      "Loss:  0.7378896391059746\n",
      "Loss:  0.7374232028430917\n",
      "Loss:  0.7369585723201854\n",
      "Loss:  0.736495853390682\n",
      "Loss:  0.7360351501477618\n",
      "Loss:  0.7355765647974603\n",
      "Loss:  0.7351201975365613\n",
      "Loss:  0.7346661464356402\n",
      "Loss:  0.7342145073275808\n",
      "Loss:  0.7337653737018577\n",
      "Loss:  0.7333188366048429\n",
      "Loss:  0.7328749845463645\n",
      "Loss:  0.7324339034127185\n",
      "Loss:  0.7319956763862987\n",
      "Loss:  0.7315603838719832\n",
      "Loss:  0.7311281034303856\n",
      "Loss:  0.7306989097180462\n",
      "Loss:  0.7302728744346104\n",
      "Loss:  0.7298500662770161\n",
      "Loss:  0.7294305509006715\n",
      "Loss:  0.7290143908875932\n",
      "Loss:  0.7286016457214312\n",
      "Loss:  0.728192371769293\n",
      "Loss:  0.7277866222702498\n",
      "Loss:  0.7273844473303809\n",
      "Loss:  0.7269858939241983\n",
      "Loss:  0.7265910059022687\n",
      "Loss:  0.726199824004825\n",
      "Loss:  0.7258123858811593\n",
      "Loss:  0.7254287261145546\n",
      "Loss:  0.7250488762525129\n",
      "Loss:  0.7246728648420178\n",
      "Loss:  0.7243007174695661\n",
      "Loss:  0.7239324568056883\n",
      "Loss:  0.7235681026536754\n",
      "Loss:  0.7232076720022271\n",
      "Loss:  0.7228511790817239\n",
      "Loss:  0.7224986354238395\n",
      "Loss:  0.7221500499241924\n",
      "Loss:  0.7218054289077525\n",
      "Loss:  0.7214647761967131\n",
      "Loss:  0.7211280931805428\n",
      "Loss:  0.7207953788879411\n",
      "Loss:  0.7204666300604263\n",
      "Loss:  0.7201418412272895\n",
      "Loss:  0.7198210047816576\n",
      "Loss:  0.7195041110574227\n"
     ]
    }
   ],
   "source": [
    "nn.fit(data, y, 0.1, epoch = 100)\n",
    "#nn.del_backward_pass(2,.06665)\n",
    "#nn.del_backward_pass(2,)\n",
    "#nn.del_backward_pass( nn.biclass_cross_entropy((nn.forward_pass(data)) , y) )\n",
    "\n",
    "#nn.del_forward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[145.14039977  86.16445243   5.02503252]]\n",
      "[[179.55169657  41.61452801]]\n",
      "[[123.74798459]]\n"
     ]
    }
   ],
   "source": [
    "del_forw = nn.del_forward_pass()\n",
    "del_back = nn.del_backward_pass(0.75)\n",
    "parameters = {}\n",
    "#for i in range(1, self.layer_count):\n",
    "print((del_forw['layer: ' + str(1)]['db'] * del_back['layer: ' + str(1)]['db']))\n",
    "print((del_forw['layer: ' + str(2)]['db'] * del_back['layer: ' + str(2)]['db']))\n",
    "print((del_forw['layer: ' + str(3)]['db'] * del_back['layer: ' + str(3)]['db']))\n",
    "\n",
    "#nn.del_backward_pass(0.75)\n",
    "\n",
    "#for i in range(3):\n",
    " #   print(del_forw['layer: '+str(i+1)]['dW'].shape)\n",
    "\n",
    "#for i in range(3):\n",
    " #   print(del_back['layer: '+str(i+1)]['dW'].shape)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
